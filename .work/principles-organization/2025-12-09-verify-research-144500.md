# Research Report: Software Engineering Principle Organization Frameworks

## Metadata
- Date: 2025-12-09
- Researcher: research-agent
- Scope: How established software engineering frameworks organize principles, with focus on identifying natural categorization patterns for 7 principles (Simple, Consistent, Documented, Tested, Debugged, Validated, Reviewed)

## Research Questions
1. How do well-known frameworks organize principles? (SOLID, Unix philosophy, Zen of Python, etc.)
2. What categories do they typically use?
3. What makes a principle "top-level" vs "supporting"?
4. Is there a natural 3-way split that aligns with established thinking?
5. What's the relationship between "quality" principles (tested, validated) and "style" principles (simple, consistent)?
6. Should process principles (reviewed, debugged) be separate from product principles?

## Key Findings

### Finding 1: Most Frameworks Use 5-9 Principles Without Higher-Level Categories

- **Source:** Multiple frameworks (SOLID, GRASP, CUPID, Twelve-Factor App)
- **Evidence:**
  - SOLID: 5 principles (Single Responsibility, Open/Closed, Liskov Substitution, Interface Segregation, Dependency Inversion)
  - GRASP: 9 patterns (Information Expert, Creator, Controller, Low Coupling, High Cohesion, Polymorphism, Pure Fabrication, Indirection, Protected Variations)
  - CUPID: 5 properties (Composable, Unix philosophy, Predictable, Idiomatic, Domain-based)
  - Twelve-Factor App: 12 factors organized into 5 implicit categories (Code Management, Config & Backing Services, Build & Release, Runtime, Parity & Operations)
- **Confidence:** HIGH
- **Implication:** Most frameworks present principles as a flat list without explicit higher-level groupings, suggesting that 7 principles could work as a flat structure

### Finding 2: When Categories Exist, They're Domain-Specific Rather Than Universal

- **Source:** Twelve-Factor App, ISO 25010, Code Complete
- **Evidence:**
  - Twelve-Factor App groups by deployment lifecycle: Code Management → Build/Release → Runtime → Operations
  - ISO 25010 uses two dimensions: Product Quality (8 characteristics) vs Quality in Use (5 characteristics)
  - Code Complete distinguishes External Quality (user-facing: correctness, usability, efficiency, reliability, robustness) vs Internal Quality (developer-facing: maintainability, readability, testability)
- **Confidence:** HIGH
- **Implication:** Categories tend to be specific to the framework's domain (deployment, quality assurance, construction). No universal "correct" categorization exists.

### Finding 3: Unix Philosophy Uses 3-Tier Hierarchy (Core → Extended → Rules)

- **Source:** Unix Philosophy (McIlroy, Gancarz, Raymond)
- **Evidence:**
  - McIlroy's 3 foundational principles (do one thing well, work together, text streams)
  - Gancarz's 9 tenets expanding on the core 3
  - Raymond's 17 rules providing detailed guidance
  - This represents a 3-tier hierarchy: Philosophy (3) → Tenets (9) → Rules (17)
- **Confidence:** HIGH
- **Implication:** Unix philosophy demonstrates successful 3-tier organization where top-level principles are most essential/memorable, with supporting details below

### Finding 4: "Three Pillars" Pattern Common in Software Engineering

- **Source:** Multiple software quality frameworks
- **Evidence:**
  - QA framework: Quality Assurance, Quality Control, Testing
  - Another QA framework: Coaching, Support, Verification
  - Agile Quality: Development/Test Automation, Software Testing, Cross-functional Team Practices
  - Software Delivery: People, Technology, Process
  - Engineering Effectiveness: Developer Productivity, Developer Experience, Business Outcomes
- **Confidence:** MEDIUM-HIGH
- **Implication:** Three-category grouping is a recognized pattern in software engineering, suggesting cultural/cognitive preference for triadic structures

### Finding 5: Process vs Product Distinction is Well-Established

- **Source:** ISO 25010, Code Complete, CUPID
- **Evidence:**
  - ISO 25010: Product Quality (inherent characteristics) vs Quality in Use (outcome of interaction)
  - Code Complete: External Quality (what users see) vs Internal Quality (how developers work)
  - CUPID vs SOLID: "CUPID describes the properties that the finished code should have... SOLID is arguably a little more restrictive, telling you how to code. Whereas CUPID is more free—however you get there is up to you, as long as the finished product has its five properties."
- **Confidence:** HIGH
- **Implication:** Distinguishing "what the code IS" (product/properties) from "how we CREATE it" (process/principles) is a recognized pattern

### Finding 6: Temporal/Lifecycle Categories Reflect SDLC Phases

- **Source:** SDLC models, Twelve-Factor App
- **Evidence:**
  - SDLC phases: Planning → Analysis → Design → Development → Testing → Implementation → Maintenance
  - Twelve-Factor App implicitly follows deployment lifecycle: Code → Config → Build → Run → Maintain
  - Agile Quality Pillars span entire lifecycle: automation (before), testing (during), collaboration (throughout)
- **Confidence:** HIGH
- **Implication:** Temporal organization (when in lifecycle) is another established categorization approach

### Finding 7: Quality Models Distinguish Internal/External and In-Use Characteristics

- **Source:** ISO 25010, McConnell's Code Complete
- **Evidence:**
  - ISO 25010 Product Quality: 8 characteristics (functional suitability, reliability, performance efficiency, usability, security, compatibility, maintainability, portability) = inherent to code
  - ISO 25010 Quality in Use: 5 characteristics (effectiveness, efficiency, satisfaction, freedom from risk, context coverage) = emerges from usage
  - Code Complete Internal Quality: maintainability, readability, testability = developer-facing
  - Code Complete External Quality: correctness, usability, efficiency, reliability, robustness = user-facing
- **Confidence:** HIGH
- **Implication:** Distinction between intrinsic/extrinsic or internal/external quality is well-established in software quality models

### Finding 8: Principles vs Properties Distinction (SOLID vs CUPID)

- **Source:** CUPID documentation, Dan North's blog
- **Evidence:**
  - "Dan North realized that the idea of principles itself was problematic. Principles are like rules: you are either compliant or you are not. Instead, he started thinking about properties: qualities or characteristics of code rather than rules to follow."
  - "Properties define a goal or centre to move towards. Your code is only closer to or further from the centre, and there is always a clear direction of travel."
  - "One clear difference from SOLID is that while SOLID is a set of principles to use during development, CUPID describes the properties that the finished code should have. One focuses on process, the other on results."
- **Confidence:** HIGH
- **Implication:** There's a meaningful distinction between prescriptive principles (how to work) and descriptive properties (desired outcomes)

### Finding 9: GRASP Splits Into "Quality Principles" vs "Design Patterns"

- **Source:** GRASP framework documentation
- **Evidence:**
  - Quality-oriented principles: Low Coupling, High Cohesion (evaluative/continuous)
  - Responsibility-assignment patterns: Information Expert, Creator, Controller, Polymorphism, Pure Fabrication, Indirection, Protected Variations (binary/applied)
  - This creates an implicit 2-tier structure: quality goals + assignment strategies
- **Confidence:** MEDIUM-HIGH
- **Implication:** Some frameworks distinguish continuous quality goals from discrete application patterns

### Finding 10: Flat Structure Works Best When Principles Are Co-Equal

- **Source:** Zen of Python, Clean Code
- **Evidence:**
  - Zen of Python: 19 aphorisms, all presented as peers (though "Flat is better than nested" is itself a principle)
  - Clean Code: Presents principles in chapters (Meaningful Names, Functions, Comments, Formatting, Objects and Data Structures, Error Handling, etc.) without hierarchical grouping
  - SOLID: 5 principles presented as equals, though often taught in sequence
- **Confidence:** MEDIUM
- **Implication:** When principles are genuinely co-equal and non-hierarchical, flat presentation may be most honest

## Patterns Observed

### Pattern A: Flat Lists (5-12 Items) Are Most Common
Most frameworks present 5-12 principles/properties as a flat list without explicit categorization. Examples: SOLID (5), GRASP (9), CUPID (5), Twelve-Factor (12), Zen of Python (19).

### Pattern B: When Categories Exist, 3-Way Splits Dominate
When frameworks do categorize, three groups is the most common structure:
- Unix: Core (3) → Tenets (9) → Rules (17)
- QA: Assurance → Control → Testing
- Agile Quality: Automation → Testing → Collaboration
- Delivery: People → Technology → Process
- Effectiveness: Productivity → Experience → Outcomes

### Pattern C: Process vs Product Is a Recognized Dichotomy
Multiple frameworks distinguish "how we work" from "what we produce":
- SOLID (process) vs CUPID (product properties)
- ISO 25010: Product Quality vs Quality in Use
- Code Complete: Internal vs External Quality

### Pattern D: Temporal/Lifecycle Organization Reflects SDLC
Some frameworks organize by "when" in the development lifecycle:
- Twelve-Factor: Code → Config → Build → Run → Maintain
- SDLC: Plan → Design → Develop → Test → Deploy → Maintain
- Agile Quality: Before (automation), During (testing), Throughout (collaboration)

### Pattern E: Quality Models Use Multi-Dimensional Frameworks
Quality-focused frameworks use 2D or 3D categorization:
- ISO 25010: Product Quality (8) × Quality in Use (5)
- Code Complete: Internal vs External × (Correctness, Usability, Efficiency, Reliability, Maintainability, Testability)
- Three Pillars: People × Technology × Process

## Analysis: Applying Patterns to CipherPowers' 7 Principles

Given the 7 principles (Simple, Consistent, Documented, Tested, Debugged, Validated, Reviewed), here are potential categorization schemes based on established patterns:

### Option 1: Flat List (Following SOLID/CUPID pattern)
Present all 7 as co-equal principles without categories.
- **Pro:** Simplest, most common pattern
- **Con:** Misses opportunity to show relationships

### Option 2: Three Pillars - Temporal/Process View
Based on "when" principles apply:

**Before Coding (Design/Planning):**
- Simple: Architecture decisions
- Consistent: Pattern selection

**During Coding (Implementation):**
- Documented: As you write
- Tested: TDD, continuous validation

**After Coding (Verification/Quality):**
- Debugged: Investigation and fixes
- Validated: Multi-layer checking
- Reviewed: Peer verification

**Pro:** Aligns with SDLC phases, intuitive flow
**Con:** Principles don't cleanly separate this way (e.g., documentation happens throughout)

### Option 3: Three Pillars - Code Quality Dimensions
Based on Code Complete's quality model:

**Internal Quality (Developer Experience):**
- Simple: Easy to understand
- Consistent: Easy to predict
- Documented: Easy to learn

**External Quality (Product Behavior):**
- Tested: Provably correct
- Validated: Robustly defended
- Debugged: Systematically investigated

**Process Quality (Team Practice):**
- Reviewed: Collectively verified

**Pro:** Aligns with established quality models
**Con:** Uneven distribution (3-3-1)

### Option 4: Three Pillars - Creation/Verification/Maintenance
Based on software lifecycle:

**Creation Principles (Writing Code):**
- Simple: Minimize complexity
- Consistent: Follow conventions
- Documented: Explain intent

**Verification Principles (Proving Correctness):**
- Tested: Automated verification
- Validated: Defense-in-depth
- Reviewed: Human verification

**Investigation Principles (Understanding Problems):**
- Debugged: Root cause analysis

**Pro:** Clean functional separation
**Con:** Uneven (3-3-1) and "Debugged" feels isolated

### Option 5: Two Dimensions - Product × Process
Based on CUPID vs SOLID distinction:

**Product Properties (What code IS):**
- Simple
- Consistent
- Documented

**Process Practices (How we ENSURE quality):**
- Tested
- Debugged
- Validated
- Reviewed

**Pro:** Aligns with SOLID/CUPID and ISO 25010 patterns
**Con:** Not a 3-way split; uneven distribution (3-4)

### Option 6: Three Pillars - Write/Verify/Fix
Based on development workflow:

**Write Well:**
- Simple
- Consistent
- Documented

**Verify Thoroughly:**
- Tested
- Validated
- Reviewed

**Debug Systematically:**
- Debugged

**Pro:** Workflow-oriented, actionable
**Con:** Uneven (3-3-1)

### Option 7: Three Pillars - Static/Dynamic/Collaborative
Based on verification approach:

**Static Properties (Inherent to Code):**
- Simple: Code structure
- Consistent: Pattern adherence
- Documented: Inline explanations

**Dynamic Verification (Runtime Behavior):**
- Tested: Automated checks
- Validated: Multi-layer defense
- Debugged: Investigation under execution

**Collaborative Quality (Human Process):**
- Reviewed: Peer verification

**Pro:** Distinguishes objective vs subjective verification
**Con:** Uneven distribution; "Debugged" classification unclear

## Gaps and Uncertainties

### Gap 1: No Universal Standard for Principle Categorization
I found no authoritative standard that prescribes how to categorize development principles. Each framework creates domain-specific categories.

### Gap 2: Unclear Whether 7 Principles Are Truly Co-Equal
Cannot determine from research whether CipherPowers' 7 principles have implicit hierarchy or are genuinely co-equal. If some are "more fundamental" than others, that should influence categorization.

### Gap 3: CipherPowers Context Not Fully Explored
Research focused on external frameworks. Did not analyze:
- How CipherPowers currently uses these principles
- Whether certain principles are emphasized more in practice
- What user/developer feedback suggests about mental models

### Gap 4: No Data on Cognitive Load of 7 Items
Found "three pillars" pattern and 5-12 item flat lists, but no research on optimal list length or cognitive chunking for principle memorization.

### Gap 5: Relationship Between "Debugged" and Other Principles Unclear
All categorization attempts struggled with "Debugged" placement:
- Is it a verification practice (like Tested/Validated)?
- Is it an investigation skill (separate category)?
- Is it a reactive principle (vs. proactive others)?

### Gap 6: "Reviewed" Could Be Meta-Principle
Reviewing potentially applies to ALL other principles (reviewing for simplicity, consistency, test coverage, etc.). Unclear if it should be categorized with them or treated as meta-level.

### Gap 7: Missing Hierarchical Importance Weighting
Research found examples of 3-tier hierarchies (Unix: 3 → 9 → 17) but no framework that explicitly weights principles by importance within flat lists. If some principles are "more important," categorization should reflect this.

## Summary

Software engineering frameworks predominantly use **flat lists of 5-12 principles** without explicit categorization. When categories exist, they're domain-specific rather than universal, with **three-way splits being the most common pattern** (following "three pillars" convention seen across QA, Agile, and organizational frameworks).

**Key categorization patterns found:**
1. **Process vs Product**: How we work vs What we produce (SOLID vs CUPID, ISO 25010 Internal vs External)
2. **Temporal/Lifecycle**: When in SDLC (Twelve-Factor, SDLC phases)
3. **Quality Dimensions**: Internal/External/In-Use (ISO 25010, Code Complete)
4. **Three Pillars**: People/Technology/Process, or Assurance/Control/Testing

**For CipherPowers' 7 principles**, the research suggests several viable approaches:

**Most aligned with established patterns:**
- **Flat list** (following SOLID/CUPID): Simple, well-precedented
- **Product Properties vs Process Practices** (3 + 4): Aligns with CUPID/SOLID distinction
- **Write/Verify/Fix** (3 + 3 + 1): Workflow-oriented, though uneven

**Challenges:**
- All 3-way splits result in uneven distribution (3-3-1 or 3-4 split)
- "Debugged" is difficult to categorize (reactive vs proactive? verification vs investigation?)
- "Reviewed" may be meta-principle applying to all others

**Key insight from CUPID:** Distinction between **prescriptive principles** (how to work) and **descriptive properties** (desired outcomes) may be relevant. CipherPowers' 7 mix both: Simple/Consistent/Documented describe code properties, while Tested/Validated/Reviewed/Debugged describe verification processes.

## Recommendations

1. **Consider Two-Tier Structure**: Top-level categories (if any) + 7 principles
2. **Test Mental Models**: User research on how developers naturally group these 7 principles
3. **Embrace Flat Structure**: Given that 7 items is within typical range (5-12), a flat list may be most honest
4. **If Categorizing, Use Product/Process Split**: This aligns best with established patterns (SOLID/CUPID, ISO 25010)
5. **Consider Temporal Alternative**: Write Well → Verify Thoroughly → Debug Systematically has workflow logic
6. **Don't Force Symmetry**: Uneven groups (3-3-1 or 3-4) are acceptable if they reflect true relationships

## Further Research Needed

1. **CipherPowers Usage Analysis**: How are these principles currently referenced in skills/agents/commands?
2. **User Mental Models**: How do CipherPowers users naturally group these principles?
3. **Principle Dependency Graph**: Are there prerequisite relationships? (e.g., must be Tested before you can Debug?)
4. **Frequency Analysis**: Which principles are invoked most often? Should inform prominence.
5. **Cross-Reference with Skills**: Do CipherPowers skills naturally cluster around certain principles?
